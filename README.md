# Enriched Baselines for Prefix-Level Session Classification: Limits and Diagnostics

This repository contains the code and documentation for **Experiment 2** of a research project on prefixâ€‘based session type prediction.  
Building on the baseline established in [session-classification-baseline](https://github.com/dgizdevans/session-classification-baseline), this experiment investigates whether adding **interâ€‘event time intervals** and **global session context** (device, geo, traffic source) improves early classification of eâ€‘commerce sessions.

All large artifacts (datasets, trained models, prediction files) are **not** stored in GitHub. They are available via Google Drive â€“ see the [Reproducibility](#-reproducibility) section below.

## ğŸ“ Repository Structure
```
.
â”œâ”€â”€ exp2.ipynb          # Main notebook with the full experiment pipeline
â”œâ”€â”€ README.md           # This file
â””â”€â”€ requirements.txt    # Python dependencies
```

## Experiment Overview

| **Data**        | BigQuery public GA4 eâ€‘commerce sample (`bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`) |
|-----------------|---------------------------------------------------------------------------------------------------------|
| **Sessions**    | 360,129 sessions defined by `(user_pseudo_id, ga_session_id)`                                           |
| **Classes**     | Buyer / Intent / Researcher / Browser (ruleâ€‘based hierarchy)                                             |
| **Task**        | Given the first `t` events of a session, predict the final session type (prefixâ€‘level classification)  |
| **Input**       | Event tokens + **Î”t** (interâ€‘event intervals) + **global context** (device, geo, traffic source)       |
| **Models**      | â€¢ Markovâ€‘1,â€‘2,â€‘3 (stepâ€‘wise backoff)<br>â€¢ LightGBM (engineered features, SHAP analysed)<br>â€¢ SASRec (transformer) with ablations (Base / +Time / +Context / +Time+Context) |
| **Split**       | Temporal 70/15/15 (based on `session_end_ts`)                                                           |
| **Metric**      | Macroâ€‘F1 (unweighted average over the four classes) computed on **all prefixes** of the test set       |

##  Results Summary (test set)

| Model              | Macroâ€‘F1 | Buyer F1 | Intent F1 | Researcher F1 | Browser F1 |
|--------------------|----------|----------|-----------|---------------|------------|
| LightGBM           | **0.5431** | 0.236    | 0.504     | 0.556         | 0.877      |
| SASRec Base        | 0.4570   | 0.209    | 0.460     | 0.450         | 0.710      |
| Markovâ€‘3           | 0.4221   | 0.111    | 0.284     | 0.442         | 0.852      |

**SASRec ablation results** (val Macroâ€‘F1, mean Â± std over 5 runs):  
| Variant         | Val Macroâ€‘F1 |
|-----------------|--------------|
| Base            | 0.4494 Â± 0.0045 |
| +Time           | 0.4484 Â± 0.0063 |
| +Context        | 0.4328 Â± 0.0070 |
| +Time+Context   | 0.4310 Â± 0.0095 |

*Detailed perâ€‘class scores, confusion matrices, and prefixâ€‘length analyses are available in the notebook and the artefacts linked below.*

## Key Findings

* **LightGBM** with engineered temporal and contextual features achieves the highest Macroâ€‘F1 (**0.5431**), outperforming both Markovâ€‘3 and SASRec. SHAP analysis confirms that productâ€‘interaction features (`count_view_item`, `count_add_to_cart`) are the primary drivers.
* **Adding time intervals and global context did *not* improve SASRec** â€“ the base model (tokens only) performed best among the transformer variants. The extra signals introduced training instability and slightly degraded overall performance.
* **Buyerâ€‘Intent ambiguity** remains the hardest challenge for all models, especially on short prefixes (`t < 10`). LightGBM shows Buyerâ†’Intent as the largest longâ€‘prefix error flow; SASRec overâ€‘predicts Buyer.
* **Error analysis** reveals distinct behavioural biases:
  * Markovâ€‘3 collapses minority classes into **Browser** (lowest error rate on short prefixes, but severe degradation after `t â‰ˆ 10`).
  * LightGBM balances precision/recall well, but Buyerâ€‘Intent confusion is its dominant longâ€‘prefix error.
  * SASRec overâ€‘predicts **Buyer**, achieving higher recall for the minority class at the cost of low precision.
* **Labelâ€‘support shift** with prefix length strongly influences performance dynamics: Browser dominates short prefixes (â‰ˆ89% at `t â‰¤ 5`), while longer prefixes are dominated by Buyer/Intent/Researcher. This shift explains much of the Macroâ€‘F1 increase with `t`.

##  Reproducibility

###  Preâ€‘computed artefacts

All data, trained models, and prediction files are available in a Google Drive folder:

ğŸ‘‰ **[Link to Google Drive folder](https://drive.google.com/drive/folders/18h8f1za8S3TEbUOnJHn3jLKWjyjV57rr?usp=sharing)** ğŸ‘ˆ

The folder contains:
- Sessionâ€‘level dataset (`sessions.parquet`)
- Vocabulary and preprocessing artefacts
- Trained LightGBM and SASRec models
- Testâ€‘set predictions for all models
- Ablation summaries and confusion matrices

Download the entire folder or individual files as needed.

### âš™ï¸ Key versioning parameters

The following constants are fixed for Experiment 2 to ensure reproducibility:

| Parameter                 | Value                  | Description                              |
|---------------------------|------------------------|------------------------------------------|
| `T_MAX`                   | 43                     | Maximum prefix length (p95 train length) |
| `SEED`                    | 42                     | Base seed for SASRec multiâ€‘run (5 runs with seeds 42,43,44,45,46) |
| `TPESampler(seed=42)`     | 42                     | Optuna sampler seed                       |
| `Optuna n_jobs`           | 1                      | Sequential trials for reproducibility     |
| `LightGBM n_jobs`         | 1                      | Deterministic training                    |

Class labels: **Buyer=0**, **Intent=1**, **Researcher=2**, **Browser=3**.

###  Running the notebook

1. Clone the repository.
2. Install dependencies: `pip install -r requirements.txt`
3. Download required artefacts from the Google Drive link above into a local folder (e.g., `./exp2_artifacts/`).
4. Open `exp2.ipynb` in Jupyter / Colab.
5. **For a full reâ€‘run** (â‰ˆ6â€‘8 hours, GPU needed for SASRec), remove the two â€œSTOPâ€ cells (before Optuna tuning and before the SASRec training loop).  
   If you only want to evaluate, you can load the preâ€‘computed predictions and skip the training sections.

## Dependencies

Key packages (full list in `requirements.txt`):
- `torch` â‰ˆ 2.0+
- `scikitâ€‘learn` â‰ˆ 1.2+
- `lightgbm` â‰ˆ 4.0+
- `pandas` â‰ˆ 2.0+
- `numpy` â‰ˆ 1.23+
- `matplotlib` â‰ˆ 3.6+
- `googleâ€‘cloudâ€‘bigquery` â‰ˆ 3.0+
- `googleâ€‘cloudâ€‘storage` â‰ˆ 2.0+
- `optuna` â‰ˆ 3.0+

##  License

[MIT](LICENSE)


* Google Cloud for the public GA4 sample dataset.
* The openâ€‘source community for the incredible tools used in this work.
